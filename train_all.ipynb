{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\r\n",
    "import numpy as np\r\n",
    "import pandas as pd \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import pickle\r\n",
    "import brian2 as br\r\n",
    "from time import time\r\n",
    "from Reservoir.reservoir import Reservoir\r\n",
    "import torchvision.datasets as datasets\r\n",
    "from sklearn.preprocessing import normalize as norm\r\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr_res(image):\r\n",
    "    return image.resize((10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [05:05, 32480.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 177974.53it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1649664it [00:49, 33380.09it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5120it [00:00, ?it/s]                   \n",
      "WARNING    C:\\Users\\hi\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      " [py.warnings]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dat = datasets.MNIST(root='./data', train=True, download=True, transform=tr_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.Series(y)\r\n",
    "y=np.array(y.replace({11:0,13:1,211:2,321:3,2212:4}))\r\n",
    "particle_name={0:'electron',1:'muon',2:'pion',3:'kaon',4:'proton'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(X,y,n):\r\n",
    "    fig,axes=plt.subplots(int(np.floor(n/4)),4,figsize=(14,int(np.floor(n/4))*8))\r\n",
    "    \r\n",
    "    #sampling examples from data\r\n",
    "    for i in range(n):\r\n",
    "        sample_id=np.random.choice([i for i in range(len(os.listdir(directory)))])\r\n",
    "        \r\n",
    "        #plotting image with particle name:\r\n",
    "        plt.subplot(np.floor(n/4),4,i+1)\r\n",
    "        plt.imshow(X[i],cmap='gray')\r\n",
    "        plt.title(f'{particle_name[y[i]]}')\r\n",
    "        plt.colorbar(label='intensity',\r\n",
    "                     fraction=0.4,orientation='horizontal') \r\n",
    "        plt.axis('off')\r\n",
    "        \r\n",
    "    plt.tight_layout()\r\n",
    "    plt.grid('off')\r\n",
    "    plt.show()\r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "pion        906047\nkaon        154323\nproton      111730\nelectron      3138\nmuon          1237\ndtype: int64"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([particle_name[i] for i in y]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1176475, 10, 10)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_two_c = X[y <= 1]\r\n",
    "y_two_c = y[y <= 1]\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_two_c = X_two_c.reshape((X_two_c.shape[0], 1, X_two_c.shape[1], X_two_c.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val=train_test_split(X,y,test_size=0.3,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "from torch import dtype, max_pool2d, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_classifier(nn.Module):\r\n",
    "    def __init__(self, output_size, device=\"cpu\"):\r\n",
    "        super(CNN_classifier, self).__init__()\r\n",
    "        self.conv = nn.Sequential(\r\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1), \r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Conv2d(16, 1, kernel_size=3, padding=1),\r\n",
    "            nn.ReLU(),\r\n",
    "            nn.Flatten()\r\n",
    "        )\r\n",
    "        self.proj = nn.Sequential(\r\n",
    "            nn.Linear(100, 25), nn.ReLU(),\r\n",
    "            nn.Linear(25, 25), nn.ReLU(),\r\n",
    "            nn.Linear(25, output_size), nn.Softmax()\r\n",
    "        )\r\n",
    "    def forward(self, im):\r\n",
    "        x = self.conv(im)\r\n",
    "        return self.proj(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\r\n",
    "  'Characterizes a dataset for PyTorch'\r\n",
    "  def __init__(self, x, labels):\r\n",
    "        'Initialization'\r\n",
    "        self.labels = labels\r\n",
    "        self.x = x\r\n",
    "\r\n",
    "  def __len__(self):\r\n",
    "        'Denotes the total number of samples'\r\n",
    "        return len(self.x)\r\n",
    "\r\n",
    "  def __getitem__(self, index):\r\n",
    "        'Generates one sample of data'\r\n",
    "\r\n",
    "        # Load data and get label\r\n",
    "        X = self.x[index]\r\n",
    "        y = self.labels[index]\r\n",
    "\r\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\r\n",
    "params = {'batch_size': 64,\r\n",
    "          'shuffle': True}\r\n",
    "\r\n",
    "use_cuda = torch.cuda.is_available()\r\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\r\n",
    "\r\n",
    "training_set = Dataset(x_train, y_train)\r\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)\r\n",
    "\r\n",
    "validation_set = Dataset(x_val, y_val)\r\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, **params)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 64,\r\n",
    "          'shuffle': True}\r\n",
    "\r\n",
    "training_generator = torch.utils.data.DataLoader(dat, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n",
      "Epoch 10/300, train loss -0.7055921417971452, val loss -0.7404870220593044, time 0.029320927460988362 ETA 0.8796278238296509\n",
      "Epoch 20/300, train loss -0.7066203604141871, val loss -0.7405978242556254, time 0.05521325270334879 ETA 0.8281987905502319\n",
      "Epoch 30/300, train loss -0.7063559467593828, val loss -0.7428037410690671, time 0.08170023759206137 ETA 0.8170023759206136\n",
      "Epoch 40/300, train loss -0.7065667286515236, val loss -0.7435374089649746, time 0.10732747713724772 ETA 0.8049560785293579\n",
      "Epoch 50/300, train loss -0.7066398821771145, val loss -0.7428539679163978, time 0.13435569206873577 ETA 0.8061341524124146\n",
      "Epoch 60/300, train loss -0.7068876139819622, val loss -0.7442595987092882, time 0.1608270208040873 ETA 0.8041351040204366\n",
      "Epoch 70/300, train loss -0.7066502397259077, val loss -0.7428663231077648, time 0.18633561531702678 ETA 0.7985812085015433\n",
      "Epoch 80/300, train loss -0.7066525717576345, val loss -0.7428691273643857, time 0.21303356091181438 ETA 0.7988758534193039\n",
      "Epoch 90/300, train loss -0.7066540407637755, val loss -0.7414730616978237, time 0.24093184471130372 ETA 0.8031061490376791\n",
      "Epoch 100/300, train loss -0.7067153217891852, val loss -0.741474239599137, time 0.2679245074590047 ETA 0.8037735223770142\n",
      "Epoch 110/300, train loss -0.706776287406683, val loss -0.7428730130195618, time 0.2949621081352234 ETA 0.8044421130960638\n",
      "Epoch 120/300, train loss -0.7065356547633806, val loss -0.7435725558371771, time 0.3223877231280009 ETA 0.8059693078200022\n",
      "Epoch 130/300, train loss -0.7064154197772344, val loss -0.742874006430308, time 0.35120767752329507 ETA 0.8104792558229886\n",
      "Epoch 140/300, train loss -0.7065362247327963, val loss -0.7421753661973136, time 0.38025460243225095 ETA 0.8148312909262521\n",
      "Epoch 150/300, train loss -0.7067775366206964, val loss -0.7442724449293954, time 0.4063608686129252 ETA 0.8127217372258504\n",
      "Epoch 160/300, train loss -0.707018818706274, val loss -0.7428747160094125, time 0.43257177670796715 ETA 0.8110720813274384\n",
      "Epoch 170/300, train loss -0.7068380986650785, val loss -0.7407780034201485, time 0.46360135873158775 ETA 0.8181200448204489\n",
      "Epoch 180/300, train loss -0.706838171929121, val loss -0.7421760360399882, time 0.4942863980929057 ETA 0.8238106634881761\n",
      "Epoch 190/300, train loss -0.7069588030378023, val loss -0.7400792383012318, time 0.5262208024660746 ETA 0.8308749512622231\n",
      "Epoch 200/300, train loss -0.7068985613683859, val loss -0.7435740346000308, time 0.5514016270637512 ETA 0.8271024405956269\n",
      "Epoch 210/300, train loss -0.7064765977362791, val loss -0.7428751162120274, time 0.5758565266927084 ETA 0.8226521809895834\n",
      "Epoch 220/300, train loss -0.7070794329047203, val loss -0.7449719934236436, time 0.6004240234692891 ETA 0.818760032003576\n",
      "Epoch 230/300, train loss -0.7064766238133112, val loss -0.7428751559484572, time 0.6290677428245545 ETA 0.8205231428146362\n",
      "Epoch 240/300, train loss -0.7069588949282964, val loss -0.7351866690885454, time 0.656301204363505 ETA 0.8203765054543813\n",
      "Epoch 250/300, train loss -0.7068383346001307, val loss -0.7414772624061221, time 0.6842642068862915 ETA 0.8211170482635498\n",
      "Epoch 260/300, train loss -0.7064163734515508, val loss -0.7449720388367063, time 0.7132089455922445 ETA 0.8229333987602822\n",
      "Epoch 270/300, train loss -0.706838347017765, val loss -0.7442730892272222, time 0.7386443972587585 ETA 0.8207159969541762\n",
      "Epoch 280/300, train loss -0.7068986309071382, val loss -0.7393804107393537, time 0.7634190559387207 ETA 0.8179489885057722\n",
      "Epoch 290/300, train loss -0.7064163771768411, val loss -0.7414772737593878, time 0.7880552728970845 ETA 0.8152295926521564\n",
      "Epoch 300/300, train loss -0.7068383482595285, val loss -0.7435741339411054, time 0.8119846065839131 ETA 0.8119846065839131\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0001 \r\n",
    "max_epochs = 300\r\n",
    "print(f\"Using {device}\")\r\n",
    "model = CNN_classifier(output_size=2)\r\n",
    "model.to(device)\r\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\r\n",
    "crit = nn.NLLLoss()\r\n",
    "train_losses = []\r\n",
    "val_losses = []\r\n",
    "t0 = time()\r\n",
    "for epoch in range(max_epochs):\r\n",
    "    train_losses_epoch = []\r\n",
    "    val_losses_epoch = []\r\n",
    "    # Training\r\n",
    "    i = 0\r\n",
    "    for local_batch, local_labels in training_generator:\r\n",
    "        # Transfer to GPU\r\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\r\n",
    "        pred = model(local_batch.float())\r\n",
    "        loss = crit(pred.float(), local_labels)\r\n",
    "        model.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        train_losses_epoch.append(loss.item())\r\n",
    "    '''i = 0\r\n",
    "    with torch.set_grad_enabled(False):\r\n",
    "        for local_batch, local_labels in validation_generator:\r\n",
    "            # Transfer to GPU\r\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\r\n",
    "            pred = model(local_batch.float())\r\n",
    "            loss = crit(pred.float(), local_labels)\r\n",
    "            val_losses_epoch.append(loss.item())'''\r\n",
    "    train_losses.append(np.mean(train_losses_epoch))\r\n",
    "    #val_losses.append(np.mean(val_losses_epoch))\r\n",
    "    if (epoch+1)%10==0:\r\n",
    "        print(f\"Epoch {epoch+1}/{max_epochs}, train loss {train_losses[-1]}, val loss {val_losses[-1]}, time {(time() - t0)/60} ETA {(time() - t0)*max_epochs/(epoch+1)/60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "name": "python385jvsc74a57bd0bfae3289c5bc4176813b446ea14ef17824238ab20835ca5f0635f33dc9fb0e31"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}